{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO <br> Y RECONOCIMIENTO DE PATRONES</center>\n",
    "## <center> 2do parcial, 2019</center>           \n",
    "\n",
    "La duración del parcial es de 3 horas. El parcial consta de 3 ejercicios, cuya suma total es de 100 puntos. El parcial es sin material y no está permitido acceder a Internet. Ante cualquier duda comuníquese con los docentes. \n",
    "\n",
    "Este notebook corresponde al ejercicio 1. Hay un notebook por ejercicio planteado.\n",
    "\n",
    "* [Ejercicio 1 - Redes Neuronales](#Ejercicio1) (35 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las bibliotecas que se utilizarán\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Ejercicio1\"></a>\n",
    "## Ejercicio 1: Completar la implementación de una red neuronal de dos capas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones auxiliares (Ejecutar y seguir)\n",
    "def error_relativo(x, y):\n",
    "    ''' devuelve el error relativo'''\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def calcular_gradiente_numerico_array(f, x, df, h=1e-5):\n",
    "    '''\n",
    "    Evalúa el gradiente numérico para una función que acepta un arreglo numpy y\n",
    "    devuelve un arreglo numpy.\n",
    "    '''\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "\n",
    "def calcular_gradiente_numerico(f, x, verbose=True, h=0.00001):\n",
    "    '''\n",
    "    Evalúa el gradiente numérico de f en x\n",
    "    - f es una función que recibe un solo argumente\n",
    "    - x es el punto (numpy array) en que se evalúa el gradiente\n",
    "    '''\n",
    "    \n",
    "    # se inicializa el gradiente \n",
    "    grad = np.zeros_like(x)\n",
    "    # se define un iterador sobre todos los elementos de x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # se evalúa la función en x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # se suma h al valor original de x\n",
    "        fxph = f(x) # se evalúa f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # se evalúa f(x - h)\n",
    "        x[ix] = oldval # se restaura el valor original de x\n",
    "\n",
    "        # se calcula la derivada parcial con la fórmula centrada\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) \n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se implementarán algunos de los bloques constitutivos de una red neuronal de **tres capas**. El diagrama muestra el diagrama de bloques para la red de dos capas implementada en el práctico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/diagrama_de_bloques.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se describen los bloques:\n",
    "- **Inicializar parámetros:** Inicializa los parámentros de la red. A los pesos de la capa $l$ de la red les llamaremos $W_l$, $b_l$ con $l=1,2,3$. \n",
    "- **Propagación hacia adelante:** La *propagación hacia adelante*  o *forward propagation* consiste en estimar la salida de la red a partir de la entrada. Cada nodo o capa de la red tiene un método *forward* asociado. Se proveen las implementaciones de los métodos forward asociados a los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activación \n",
    "        - Afin --> Activación\n",
    "        \n",
    "- **Loss:** Calcula el valor de la función de costo a optimizar. Se implementarán dos funciones de costo:\n",
    "        - Entropía cruzada\n",
    "        - Entropía cruzada regularizada\n",
    "- **Propagación hacia atrás:** Durante la *propagación hacia atrás* o *backpropagation* se calculan los gradientes necesarios para actualizar los parámetros de la red. Se implementarán métodos *backward* para los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activación \n",
    "        - Afin --> Activación\n",
    "- **Update:** Es el boque encargado de actualizar los parámetros. Para ello utiliza los gradientes calculados durante la *propagación hacia atrás* y un método de optimización. Se utilizará *descenso por gradiente* como método de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Bloque de Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementará el bloque de inicialización para el caso de una red neuronal de **tres capas** con la siguiente estructura:    \n",
    "  Afin --> Activación 1 --> Afin --> Activacion 2 --> Afin --> Activación 3       \n",
    "\n",
    "### Parte a) \n",
    "Completar la implementación de `inicializar_pesos()`. Los pesos $W_l$ serán inicializados en valores aleatorios con distribución gaussiana de desviación estandar $\\sigma_l=1/\\sqrt{d_{l-1}}$, siendo $d_{l-1}$ el número de nodos de la capa $l-1$. Por ejemplo, para la primera capa $l=1$, la cantidad de nodos $d_{l-1}=d_0$ corresponde a la dimensión del vector de características. Los pesos correspondientes a términos de *bias* se inicializarán a cero. \n",
    "\n",
    "**Nota:** No necesario realizar una implementación genérica. Alcanza con que funcione para una red de tres capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_pesos(dims, semilla=1):\n",
    "    \"\"\"\n",
    "    Entrada:\n",
    "        dims: lista que contiene el número de nodos de cada una de las capas. El primer elemento\n",
    "              corresponde al tamaño del vector de características y el último a la cantidad de \n",
    "              nodos en la última capa oculta.\n",
    "        semilla: semilla a utilizar para generar los valores aleatorios\n",
    "    \n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los parámetros inicializados \n",
    "                    parametros['W' + str(l)] = ... \n",
    "                    parametros['b' + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(semilla)\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    # Sugerencia: puede ser útil utilizar np.random.randn() y ajustar la desviación estándar\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "   \n",
    "    # Se genera el diccionario con los valores inicializados\n",
    "    parametros = {'W1': W1,\n",
    "                  'b1': b1,\n",
    "                  'W2': W2,\n",
    "                  'b2': b2,\n",
    "                  'W3': W3,\n",
    "                  'b3': b3}\n",
    "    \n",
    "    return parametros    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se testea la inicialización con pesos aleatorios\n",
    "dims = [3,6,3,1]\n",
    "parametros = inicializar_pesos(dims)\n",
    "\n",
    "W1_correcto = np.array([[ 0.93781623, -0.35319773, -0.3049401 , -0.61947872,  0.49964333, -1.32879399],\n",
    "                       [ 1.00736754, -0.43948301,  0.18419731, -0.14397405,  0.84414841, -1.18942279],\n",
    "                         [-0.18614766, -0.22173389,  0.65458209, -0.63502252, -0.09955147, -0.50683179]])\n",
    "b1_correcto = np.array([0., 0., 0., 0., 0., 0.])\n",
    "W2_correcto = np.array([[ 0.01723369,  0.23793331, -0.4493259 ],[ 0.4673315 ,  0.36807287,  0.20514245],\n",
    "                       [ 0.3677729 , -0.27913073, -0.05016972], [-0.38202627, -0.10936485,  0.21651671],\n",
    "                       [-0.28236932, -0.16197395, -0.28053708], [-0.34505376, -0.27403509, -0.0051703 ]])\n",
    "b2_correcto = np.array([0., 0., 0.])  \n",
    "W3_correcto = np.array([[-0.64507943],\n",
    "       [ 0.13533997],\n",
    "       [ 0.95828723]])\n",
    "b3_correcto = np.array([0.])\n",
    "\n",
    "# Se compara la salida con la nuestra. El error debería ser e-7 o menos.\n",
    "print('Testeando la incialización aleatoria:')\n",
    "print('Diferencia en W1: ', error_relativo(parametros['W1'], W1_correcto))\n",
    "print('Diferencia en b1: ', error_relativo(parametros['b1'], b1_correcto))\n",
    "print('Diferencia en W2: ', error_relativo(parametros['W2'], W2_correcto))\n",
    "print('Diferencia en b2: ', error_relativo(parametros['b2'], b2_correcto))\n",
    "print('Diferencia en W3: ', error_relativo(parametros['W3'], W3_correcto))\n",
    "print('Diferencia en b3: ', error_relativo(parametros['b3'], b3_correcto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Bloques Forward\n",
    "\n",
    "Se proveen las implementaciones de los métodos *forward* de los siguientes bloques: \n",
    "\n",
    "- Bloque Afin  \n",
    "- Bloque Activación donde la activación puede ser ReLU, Sigmoide\n",
    "- Bloque Afin -> Activación  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Forward Afin\n",
    "\n",
    "La señal de entrada a la activación de la capa $\\textit{l}$ puede escribirse como:\n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{s}^{(l)}$ y $\\mathbf{b}^{(l)}$ son vectores de tamaño $d^{(l)}$, $\\mathbf{x}^{(l-1)}$  es un vector de tamaño $d^{(l-1)}$ y $W^{(l)}$ es una matriz de tamaño $d^{(l-1)} \\times d^{(l)}$.\n",
    "\n",
    "La ecuación (1) es válida cuando la entrada a la capa es un único vector $\\mathbf{x}^{(l-1)}$. En la práctica es más habitual procesar un $\\textit{batch}$ de vectores de entrada a la vez, por lo tanto es deseable contar con una expresión que genere la salida para todos los vectores de entrada a la vez. Al evitar la utilización de un bloque $\\textit{for}$ que itere por cada una de las muestras del $\\textit{batch}$ se mejora la eficiencia de la implementación.   \n",
    "\n",
    "\n",
    "La versión de la ecuación (1) que actúa sobre un conjunto de muestras a la vez es la siguiente:\n",
    "\n",
    "$$\n",
    "S^{(l)} = X^{(l-1)}W^{(l)} +b^{(l)}\\tag{2}\n",
    "$$\n",
    "\n",
    "donde $X^{[0]} = X$, siendo X una matriz que contiene un vector de características en cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante en una capa afin.\n",
    "\n",
    "    Entrada:\n",
    "        X: matriz de tamaño (N, dim capa anterior) que en cada fila contiene un vector de\n",
    "           activaciones de la capa anterior (o datos de entrada)\n",
    "        W: matriz de pesos de tamaño (dim de capa anterior, dim de capa actual) \n",
    "        b: vector de bias de tamaño (dim de la capa actual,)\n",
    "\n",
    "    Salida:\n",
    "        S: matriz de tamaño (N, dim de capa actual) que contiene\n",
    "           los scores o señal de entrada a la activación  \n",
    "        cache: (X, W, b) tupla que contiene X, W y b. \n",
    "               Son almacenados para calcular el paso backward eficientemente\n",
    "    \"\"\"\n",
    "\n",
    "    S = np.dot(X, W) + b\n",
    "    \n",
    "    assert(S.shape == (X.shape[0], W.shape[1] ))\n",
    "    cache = (X, W, b)\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se proveen las implementaciones de las siguientes funciones de activación:\n",
    "\n",
    "- **Sigmoide**: $\\sigma(S) = \\sigma(X W  + b) = \\frac{1}{ 1 + e^{-(X W  + b)}}$. Esta función devuelve, además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = sigmoid(S)\n",
    "```\n",
    "\n",
    "\n",
    "- **Rectified Linear Unit**:  $ReLU(S) = \\max(0, S)$.  Al igual que en el caso de la activación sigmoide, esta función devuelve además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(S):\n",
    "    \"\"\"\n",
    "    Implementa la activación sigmoide\n",
    "    \n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación. \n",
    "           Las dimensiones de entrada no están definidas.\n",
    "    \n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de sigmoid(S) \n",
    "    cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    \"\"\"\n",
    "\n",
    "    X = 1/(1+np.exp(-S))\n",
    "    cache = S\n",
    "\n",
    "    assert X.shape == S.shape, 'La entrada y la salida deben ser del mismo tamaño'\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(S):\n",
    "    '''\n",
    "    Implementa la activación relu\n",
    "    \n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación. \n",
    "           Las dimensiones de entrada no están definidas.\n",
    "    \n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de relu(S) \n",
    "    cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    '''\n",
    "    \n",
    "    X = np.maximum(0,S)\n",
    "    \n",
    "    assert(X.shape == S.shape)\n",
    "    cache = S \n",
    "        \n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Aplicación conjunta de capa afin y activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se provee la implementación de la propagación hacia adelante de una capa *Afin->Activacion*. El método `afin_activacion_forward()` implementa la operación:\n",
    "\n",
    "$$\n",
    "X^{[l]} = \\theta(S^{(l)}) = \\theta(X^{(l-1)}W^{(l)} +b^{(l)})\n",
    "$$\n",
    "\n",
    "donde la activación $\\theta(\\cdot)$ será alguna de las implementadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_activacion_forward(X_prev, W, b, activacion):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante para una capa Afin->Activación \n",
    "    Entrada:\n",
    "        X_prev: arreglo de tamaño (N, dim capa anterior) que contiene la \n",
    "                activación de la capa anterior (o datos de entrada):          \n",
    "        W: matriz de pesos de tamaño (dim de capa anterior, dim de capa actual)  \n",
    "        b: vector de bias de tamaño (dim de la capa actual)\n",
    "        activacion: la activacion a utilizar en esta capa se indica con uno de los \n",
    "                    siguientes strings: 'sigmoide', 'tanh' o 'relu'\n",
    "\n",
    "    Salida:\n",
    "        X: arreglo de tamaño (N, dim de capa actual) que contiene la salida \n",
    "           de la función de activación  \n",
    "    cache: tupla que contiene \"cache_afin\" y \"cache_activacion\".\n",
    "           Se almacenan para calcular la propagación hacia atrás eficientemente\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    S, cache_afin = afin_forward(X_prev,W,b)\n",
    "    \n",
    "    if activacion == 'sigmoide':    \n",
    "        \n",
    "        X, cache_activacion = sigmoide(S)\n",
    "    \n",
    "    elif activacion == 'relu':\n",
    "        \n",
    "        X, cache_activacion = relu(S)\n",
    "    \n",
    "    assert (X.shape == (X_prev.shape[0], W.shape[1]))\n",
    "    cache = (cache_afin, cache_activacion)\n",
    "\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Función de costo\n",
    "\n",
    "En esta sección se implementarán dos funciones de costo.  \n",
    "\n",
    "- **Entropía cruzada:** Es la función de costo más utilizada en problemas de clasificación binaria. Se recuerda que la misma se define mediante la fórmula:\n",
    "$$\n",
    "H(\\mathbf{\\mathbf{x}^{(L)}}, \\mathbf{y})= -\\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left( y_n\\log x^{(L)}_n + (1-y_n)\\log\\left(1- x^{(L)}_n\\right) \\right) \\tag{3}\n",
    "$$\n",
    "\n",
    "- **Entropía cruzada regularizada:** Es la versión regularizada de la *entropía cruzada* definida anteriormente:\n",
    "$$\n",
    "H_{reg}(\\mathbf{\\mathbf{x}^{(L)}}, \\mathbf{y})= -\\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left( y_n\\log x^{(L)}_n + (1-y_n)\\log\\left(1- x^{(L)}_n\\right) \\right) + \\frac{1}{2N}\\sum_{l=1}^{L} \\Vert W_l \\Vert_2^2 \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte b) \n",
    "Implementar el método `entropia_cruzada_regularizada()`. De manera similar a la implementación de `entropia_cruzada()` provista, la función deberá devolver, además del costo, el gradiente del costo respecto al vector $\\mathbf{x}^{(L)}$ (salida de la red y entrada del bloque *Loss*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia_cruzada(xL, y):\n",
    "    \"\"\"\n",
    "    Implementa la entropía cruzada\n",
    "\n",
    "    Entrada:\n",
    "        xL: vector de dimensión (N,1) que contiene las ¨probabilidades¨ de pertenecer a la clase positiva \n",
    "            estimadas por el modelo\n",
    "        y: vector de etiquetas de dimesión (N,1) (con unos para la clase positiva y 0 para la negativa)\n",
    "\n",
    "    Salida:\n",
    "        costo: escalar con el costo calculado\n",
    "        dxL: gradiente del costo respecto a xL, tiene las mismas dimensiones que xL\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(y)\n",
    "\n",
    "    logprobs = np.log(xL) * y + (1 - y) * np.log(1 - xL)\n",
    "    costo = -np.mean(logprobs)\n",
    "    \n",
    "    dxL = -(np.divide(y, xL) - np.divide(1 - y, 1 - xL))/N\n",
    "\n",
    "    costo = np.squeeze(costo) # Para asegurarnos que la salida sea un escalar (Ej: transforma [[12]] en 12).\n",
    "    assert(costo.shape == ())\n",
    "    assert(dxL.shape == xL.shape), 'Las dimensiones de dxL y xL deben ser iguales'\n",
    "    return costo, dxL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia_cruzada_regularizada(xL, y, parametros, factor_reg):\n",
    "    \"\"\"\n",
    "    Implementa la entropía cruzada\n",
    "\n",
    "    Entrada:\n",
    "        xL: vector de dimensión (N,1) que contiene las ¨probabilidades¨ de pertenecer a la clase positiva \n",
    "            estimadas por el modelo\n",
    "        y: vector de etiquetas de dimesión (N,1) (con unos para la clase positiva y 0 para la negativa)\n",
    "        parametros: diccionario python que contiene los parametros de la red\n",
    "        factor_reg: factor de regularización\n",
    "\n",
    "    Salida:\n",
    "        costo: escalar con el costo calculado (tomando en cuenta la regularización)\n",
    "        dxL: gradiente del costo respecto a xL, tiene las mismas dimensiones que xL\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(y)\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    costo = np.squeeze(costo) # Para asegurarnos que la salida sea un escalar (Ej: transforma [[12]] en 12).\n",
    "    assert(costo.shape == ())\n",
    "    assert(dxL.shape == xL.shape), 'Las dimensiones de dxL y xL deben ser iguales'\n",
    "    return costo, dxL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se testea la implementación de la entropía cruzada regularizada\n",
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 2, 10\n",
    "xL_ = np.random.rand(num_inputs,1)\n",
    "y_ = np.random.randint(num_classes, size=(num_inputs,1))\n",
    "W1_ = np.random.randn(5, 3)\n",
    "W2_ = np.random.randn(3, 4)\n",
    "W3_ = np.random.randn(4, 1)\n",
    "param={'W1':W1_,'W2':W2_,'W3':W3_}\n",
    "lambd = 0.1\n",
    "\n",
    "dxL_num = calcular_gradiente_numerico(lambda xL: entropia_cruzada_regularizada(xL, y_, param, lambd)[0],\n",
    "                                                 xL_, verbose=False)\n",
    "costo, dxL = entropia_cruzada_regularizada(xL_, y_,  param, lambd)\n",
    "\n",
    "# Testing la entropía cruzada regularizada. El costo debería dar cercano a 1.12 y el error en dP alrededor de 1e-8\n",
    "print('\\nTesting entropia_cruzada:')\n",
    "print('costo: ', costo)\n",
    "print('error en dP: ', error_relativo(dxL_num, dxL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Propagación hacia atrás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementará la versión *backward* de cada una de las funciones *forward* implementadas anteriormente. A saber:\n",
    "- AFIN backward\n",
    "- ACTIVACION backward \n",
    "- AFIN -> ACTIVACION backward donde ACTIVACION puede ser *ReLU* o *sigmoide* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Afin backward\n",
    "\n",
    "Durante la propagación hacia adelante en la capa $l$ (sin considerar la activación) se calcula para una muestra: \n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "Si se llama $e_n$ al costo debido a la muesta $n$ y se asume conocido el *vector de sensibilidad* $\\delta^{(l)}=\\frac{\\partial e_n}{\\partial \\mathbf{s}^{(l)}}$, en el teórico del curso se vio que \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{W^{(l)}}}=\\mathbf{x}^{(l-1)} \\left( \\delta^{(l)} \\right)^T\n",
    "$$\n",
    "\n",
    "Análogamente a como se hizo en el caso de la propagación hacia adelante, si se considera la contribución al error de un conjunto de muestras a la vez la ecuación se puede escribir en forma vectorizada como:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W^{(l)}}}= dW^{(l)} = \\left( X^{(l-1)}\\right)^ T dS^{(l)}   \\tag{5}\n",
    "$$\n",
    "\n",
    "donde $dS^{(l)}$ es una matríz de tamaño $N\\times d^{(l)}$ que en cada fila contiene el vector de sensibilidad $\\delta^{(l)}_n$ correspondiente a una de las muestras.\n",
    "\n",
    "Las derivadas respecto al vector de bias $\\mathbf{b}^{(l)}$ se calculan de forma similar (puede pensarse como un caso particular en que $X^{(l-1)}$ es un vector columna de unos) por lo que\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}= d\\mathbf{b}^{(l)} =\\mathbb{1} ^ T dS^{(l)}  \\tag{6}\n",
    "$$\n",
    "\n",
    "Finalmente se calcula la influencia de cada una de las características en el error. Considerando primero el caso de una muestra, se tiene que:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{\\mathbf{x}^{(l-1)}}} = W^{(l)} \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "que en forma vectorizada puede escribirse como:\n",
    "\n",
    "$$ \n",
    " \\frac{\\partial E }{\\partial X^{(l-1)}} = dX^{(l-1)} = dS^{(l)} \\left( W^{(l) }\\right)^T \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte c) \n",
    "Implementar el método `afin_backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_backward(dS, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás para una capa l (sin considerar la activación)\n",
    "\n",
    "    Entrada:\n",
    "        dS: Gradiente de la función de costo con respecto a la salida de la capa actual \n",
    "            (sin considerar la activación)\n",
    "        cache: tupla de valores (X_prev, W, b) calculados durante la propagación hacia adelante\n",
    "               de la capa actual\n",
    "\n",
    "    Salida:\n",
    "        dX_prev: Gradiente de la función de costo con respecto a la activación de la capa anterior (l-1), \n",
    "                 tiene el mismo tamaño que X_prev\n",
    "        dW: Gradiente de la función de costo con respecto a W (de la capa actual l), \n",
    "            tiene el mismo tamaño que W\n",
    "        db: Gradiente de la función de costo con respecto a b (de la capa actual l), \n",
    "            tiene el mismo tamaño que b\n",
    "    \"\"\"\n",
    "    X_prev, W, b = cache\n",
    "    N = X_prev.shape[0]\n",
    "\n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    assert (dX_prev.shape == X_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de afin_backward\n",
    "np.random.seed(43)\n",
    "x = np.random.randn(10, 6)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "#dx_num = calcular_gradiente_numerico_array(lambda x: afin_forward(x, w, b)[0], x, dout)\n",
    "dx_num = calcular_gradiente_numerico_array(lambda xx: afin_forward(xx, w, b)[0], x, dout)\n",
    "\n",
    "dw_num = calcular_gradiente_numerico_array(lambda ww: afin_forward(x, ww, b)[0], w, dout)\n",
    "db_num = calcular_gradiente_numerico_array(lambda bb: afin_forward(x, w, bb)[0], b, dout)\n",
    "\n",
    "_, cache = afin_forward(x, w, b)\n",
    "dx, dw, db = afin_backward(dout, cache)\n",
    "\n",
    "# El error debería ser del orden de e-9 o menos\n",
    "print('Testing afin_backward():')\n",
    "print('dx error: ', error_relativo(dx_num, dx))\n",
    "print('dw error: ', error_relativo(dw_num, dw))\n",
    "print('db error: ', error_relativo(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Activación backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si  $\\theta(\\cdot)$ es la función de activación, entonces su función *backward* se calcula \n",
    "\n",
    "$$\n",
    "dS^{(l)} = dX^{(l)} * \\theta'(S^{(l)})   \\tag{8}\n",
    "$$.  \n",
    "\n",
    "donde $\\theta'(\\cdot)$ debe ser calculado para cada caso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte d) \n",
    "Implementar los métodos *backward* cada una de las funciones de activación implementadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación Sigmoide.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa relu,\n",
    "              el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Returns:\n",
    "    dS -- Gradiente del costo respecto a S\n",
    "    \"\"\"\n",
    "    \n",
    "    S = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    assert (dS.shape == S.shape), 'dS y S no tienen el mismo tamaño'\n",
    "    assert (dX.shape == S.shape), 'dX y S no tienen el mismo tamaño'\n",
    "    \n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de sigmoid_backward\n",
    "np.random.seed(231)\n",
    "S = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*S.shape)\n",
    "\n",
    "dS_num = calcular_gradiente_numerico_array(lambda S: sigmoide(S)[0], S, dout)\n",
    "\n",
    "_, cache = sigmoide(S)\n",
    "dS = sigmoide_backward(dout, cache)\n",
    "\n",
    "# El error debería ser del orden de e-10 o menos\n",
    "print('Testing relu_backward():')\n",
    "print('dS error: ', error_relativo(dS_num, dS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación ReLu.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa relu,\n",
    "              el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Returns:\n",
    "    dS -- Gradiene del costo respecto a S\n",
    "    \"\"\"\n",
    "    \n",
    "    S = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    assert (dS.shape == S.shape)\n",
    "    \n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dS_num = calcular_gradiente_numerico_array(lambda x: relu(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu(x)\n",
    "dS = relu_backward(dout, cache)\n",
    "\n",
    "# El error debería ser del orden de e-12\n",
    "print('Testing relu_backward():')\n",
    "print('dS error: ', error_relativo(dS_num, dS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Afin --> Activacion backward\n",
    "\n",
    "A continuación se implementará la función que realiza la propagación hacia atrás del la capa *Afin-->Activacion*. \n",
    "\n",
    "### Parte e) \n",
    "Implementar la función `afin_activacion_backward()`. Para ello utilizar las funciones implementadas anteriormente: `afin_backward` y la ¨`activacion_backward`¨ que corresponda. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_activacion_backward(dX, cache, activacion):\n",
    "    '''\n",
    "    Implementar la propagación hacia atrás para la capa Afin->Activacion.\n",
    "    \n",
    "    Entradas:\n",
    "        dX: gradiente del costo respecto a la salida de la capa actual \n",
    "        cache: tupla con los valores(cache_afin, cache_activacion) \n",
    "        activacion: la activación a utilizar en esta capa, puede ser 'sigmoide' o 'relu'\n",
    "    Salidas:\n",
    "        dX_prev: Gradiente del costo con respecto a la activación de la capa anterior(l-1), \n",
    "                 tiene las mismas dimensiones que X_prev\n",
    "        dW -- Gradiente del costo con respecto a W (de la capa actual l), \n",
    "              tiene las mismas dimensiones que W\n",
    "        db -- Gradiente del costo con respecto a b (de la capa actual l), \n",
    "              tiene las mismas dimensiones que b\n",
    "    '''\n",
    "    cache_afin, cache_activacion = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    if activacion == 'relu':\n",
    "\n",
    "    elif activacion == 'sigmoide':\n",
    "       \n",
    "\n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 12)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "activaciones = ['relu', 'sigmoide']\n",
    "\n",
    "for activacion in activaciones:\n",
    "    out, cache = afin_activacion_forward(x, w, b, activacion)\n",
    "    dx, dw, db = afin_activacion_backward(dout, cache, activacion)\n",
    "\n",
    "    dx_num = calcular_gradiente_numerico_array(lambda x: afin_activacion_forward(x, w, b, activacion)[0], x, dout)\n",
    "    dw_num = calcular_gradiente_numerico_array(lambda w: afin_activacion_forward(x, w, b, activacion)[0], w, dout)\n",
    "    db_num = calcular_gradiente_numerico_array(lambda b: afin_activacion_forward(x, w, b, activacion)[0], b, dout)\n",
    "\n",
    "    # Los errores deberían ser del orden de e-9 o menos\n",
    "    print('Testing afin_' + activacion + '_forward y afin_' + activacion + '_backward:')\n",
    "    print('dx error: ', error_relativo(dx_num, dx))\n",
    "    print('dw error: ', error_relativo(dw_num, dw))\n",
    "    print('db error: ', error_relativo(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5 - Actualización de los parámetros\n",
    "\n",
    "En esta sección se actualizarán los parámetros del modelo mediante el método de *descenso por gradiente*:\n",
    "\n",
    "$$ W^{(l)} = W^{(l)} -\\eta \\text{ } dW^{(l)} \\tag{9}$$\n",
    "$$ \\mathbf{b}^{(l)} = \\mathbf{b}^{(l)} -\\eta \\text{ } \\mathbf{db}^{(l)} \\tag{10}$$\n",
    "\n",
    "donde $\\eta$ es el *learning rate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte f) \n",
    "Implementar `actualizar_parametros()` para actualizar los parámetros usando *descenso por gradiente*. Luego de actualizar los parámetros, almacenarlos en el diccionario de parámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_parametros(parametros, gradientes, learning_rate):\n",
    "    \"\"\"\n",
    "    Se actualizan los parámetros utilizando descenso por gradiente. Si bien en este notebook se trabaja \n",
    "    con una red de dos capas, el método se implementa en forma genérica para mostrar como se haría en el\n",
    "    caso más general.\n",
    "    \n",
    "    Entrada:\n",
    "        parametros: diccionario de python que contiene los parámetros \n",
    "        gradientes: diccionario de python que contiene los gradientes \n",
    "                    (las salidas de los métodos backward)\n",
    "    \n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los parámetros actualizados \n",
    "                    parametros[\"W\" + str(l)] = ... \n",
    "                    parametros[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parametros) // 2 # número de capas en la red neuronal\n",
    "    \n",
    "    # Se actualiza cada uno de los parámetros. En el caso de una red profunda de L capas\n",
    "    # se hace con un loop que va recorriendo cada parámetro\n",
    "    for l in range(1,L+1):\n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "    \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación utilizando datos sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux_datos import load_2D_dataset, mostrar_frontera_decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan datos sintéticos con forma de flor pertenecientes a dos clases: $cero$ y $uno$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_2D_dataset()\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Se muestran los datos\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train.ravel(), s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red para clasificar los datos sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para clasificar los datos sintéticos se utilizará una red de **tres capas** con la siguiente arquitectura:   \n",
    "\n",
    "*Afin->Relu->Afin->Relu-->Afin-->Sigmoide* \n",
    "\n",
    "### Parte g)  \n",
    "Completar la implementación del método `red_tres_capas()` utilizando los métodos *forward* y *backward* adecuados para dicha arquitectura. Como función de costo se utilizará la *entropía cruzada*.\n",
    "\n",
    "**Nota:** La función tiene previsto un parámetro para indicar si se aplica o no regularización y otro parámetro para el factor de regularización. En esta parte no hay que utilizar ninguno de estos parámetros, o bien asumir que `regularizar` es `False`. La regularización se implementará en una parte posterior.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_tres_capas(X, Y, dims_capas, num_iter = 1000, learning_rate = 1,\n",
    "                    mostrar_costo=False, semilla=100, regularizar=False, reg_factor=1):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de dos capas: Afin->Relu->Afin->Sigmoide.\n",
    "    \n",
    "    Entrada:\n",
    "        X: datos de entrada, de tamaño (N, d_0)\n",
    "        Y: etiquetas (1 para la clase positiva y 0 para la negativa), de tamaño (N,1)\n",
    "        dims_capas: dimensiones de las capas(d_0, d_1, d_2)\n",
    "        num_iter: número de iteraciones del loop de optimización\n",
    "        learning_rate: learning rate utilizado para la actualización mediante descenso por gradiente\n",
    "        mostrar_costo: Si vale True, se muestra el costo cada 100 iteraciones \n",
    "        semilla: semilla utilizada para la generación de números aleatorios\n",
    "        regularizar: indica si se aplica o no regularización\n",
    "        reg_factor: factor de regularización\n",
    "    Salida:\n",
    "        parametros: un diccionario de python que contiene W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(semilla)\n",
    "    gradientes = {} # se inicializa el diccionario que almacena los gradientes\n",
    "    costos = []     # lista que almacena el costo\n",
    "    N = X.shape[0]  # número de muestras\n",
    "    \n",
    "    # Se inicializan los parámetros del diccionario llamando a una de las \n",
    "    # funciones previamente implementadas\n",
    "    parametros = inicializar_pesos(dims_capas, semilla=semilla)\n",
    "     \n",
    "    # Se obtienen W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "    W3 = parametros[\"W3\"]\n",
    "    b3 = parametros[\"b3\"]\n",
    "    # Loop (descenso por gradiente)\n",
    "\n",
    "    for i in range(0, num_iter):\n",
    "\n",
    "        ####################################################################################\n",
    "        ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "        \n",
    "        # Propagación hacia adelante: Afin -> Relu -> Afin -> Relu -> Afin -> Sigmoide. \n",
    "        # Entradas: \"X, W1, b1\". Salidas: \"X1, cache1, X2, cache2, X3, cache3\".\n",
    "\n",
    "        \n",
    "        \n",
    "        # Se calcula el costo y se inicia la propagación hacia atrás\n",
    "\n",
    "        \n",
    "        \n",
    "        # Propagación hacia atrás. \n",
    "        # Entradas: \"dX3, cache3, cache2, cache1\". \n",
    "        # Salidas: \"dX2, dW3, db3, dX1, dW2, db2, dW1, db1, dX0 (no utilizado)\".\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Se almacenan los gradientes recientemente calculados en el diccionario \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Se actualizan los parámetros\n",
    "        \n",
    "        \n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "\n",
    "        # Se obtienen los nuevos W1, b1, W2, b2, W3 y b3 del diccionario de parámetros.        \n",
    "        W1 = parametros[\"W1\"] \n",
    "        b1 = parametros[\"b1\"]\n",
    "        W2 = parametros[\"W2\"]\n",
    "        b2 = parametros[\"b2\"]\n",
    "        W3 = parametros[\"W3\"]\n",
    "        b3 = parametros[\"b3\"]\n",
    "\n",
    "        \n",
    "        # Se muestra la evolución del costo cada 100 iteraciones\n",
    "        if mostrar_costo and i % 1000 == 0:\n",
    "            print(\"Costo luego de iteracion {}: {}\".format(i, np.squeeze(costo)))\n",
    "\n",
    "        if mostrar_costo and i % 1000 == 0:\n",
    "            costos.append(costo)\n",
    "    \n",
    "    # se muestra el costo\n",
    "    if mostrar_costo:    \n",
    "        plt.plot(np.squeeze(costos))\n",
    "        plt.ylabel('costo')\n",
    "        plt.xlabel('iteraciones (sobre 100)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Se definen las constantes que determinan la arquitectura de la red ####\n",
    "d_0 = X_train.shape[1]   \n",
    "d_1 = 20\n",
    "d_2 = 3\n",
    "d_3 = 1\n",
    "#dims_capas = [d_0, d_1, d_2, d_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se entrena la red, con los parámetros por defecto el costo debería ser alrededor de 0.65 en la iteración 0 y \n",
    "# menor a 0.15 en la 20000\n",
    "parametros_red_3capas = red_tres_capas(X_train, Y_train, dims_capas = [d_0, d_1, d_2, d_3], \n",
    "                                    learning_rate = 0.2, num_iter = 38000, mostrar_costo=True,\n",
    "                                    regularizar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar la frontera de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte h)  \n",
    "Mostrar la frontera de decisión. Para ello se deberá completar primero la implementación del método `predecir_clase_datos_sinteticos()`. Dicho método utiliza los parámetros de la red recientemente encontrados para predecir la clase de los vectores de características pasados como parámetro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_clase_datos_sinteticos(X, parametros):\n",
    "    \"\"\"\n",
    "    Esta función predice la clase de los datos sintéticos. \n",
    "    \n",
    "    Entrada:\n",
    "        X: matriz de tamaño Nx2 que en cada fila contiene un vector de características\n",
    "        parametros: parametros del modelo ya entrenado\n",
    "    \n",
    "    Salida:\n",
    "        p : vector de tamaño Nx1 que contiene las predicciones realizadas (0 o 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se obtienen W1, b1, W2, b2, W3 y b3 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "    W3 = parametros[\"W3\"]\n",
    "    b3 = parametros[\"b3\"]\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    p = np.zeros((N,1))\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    # Se hace la propagación hacia adelante de los datos de entrada X. Tener en cuenta que la\n",
    "    # arquitectura utilizada en la red fue Afin-->Relu-->Afin-->Relu-->Afin-->Sigmoide\n",
    "    # ~ 3 lineas de codigo\n",
    "\n",
    "    \n",
    "    # Se obtienen las predicciones. Si la salida es mayor que 0.5 se asigna la clase 1, de lo \n",
    "    # contrario se asigna 0\n",
    "    # ~ 1 linea de codigo\n",
    "\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda muestra el porcentaje de acierto con el conjunto de entrenamiento. Verificar que para los parámetros por defecto es mayor al 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_train = predecir_clase_datos_sinteticos(X_train, parametros_red_3capas)\n",
    "porcentaje_aciertos = np.mean(predicciones_train==Y_train)\n",
    "print('El porcentaje de aciertos es %f' % porcentaje_aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se muestra la frontera de decisión. Verificar que es razonable para el conjunto de entrenamiento.\n",
    "mostrar_frontera_decision(lambda x: predecir_clase_datos_sinteticos(x, parametros_red_3capas), X_train, Y_train.flatten())\n",
    "plt.title('Frontera de decisión para una capa oculta de ' + str(d_1) + ' nodos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte i)\n",
    "Modificar la implementación de `red_tres_capas()` de modo que cuando se pase como parámetro `regularizar=True` utilice como función de costo `entropia_cruzada_regularizada()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Con regularización\n",
    "parametros_red_3capas_regularizada = red_tres_capas(X_train, Y_train, dims_capas = [d_0, d_1, d_2, d_3], \n",
    "                                                    learning_rate = 0.2, num_iter = 38000, mostrar_costo=True,\n",
    "                                                    regularizar=True, reg_factor=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_train = predecir_clase_datos_sinteticos(X_train, parametros_red_3capas_regularizada)\n",
    "porcentaje_aciertos = np.mean(predicciones_train==Y_train)\n",
    "print('El porcentaje de aciertos es %f' % porcentaje_aciertos)\n",
    "\n",
    "# Se muestra la frontera de decisión. Verificar que es razonable para el conjunto de entrenamiento.\n",
    "mostrar_frontera_decision(lambda x: predecir_clase_datos_sinteticos(x, parametros_red_3capas_regularizada), X_train, Y_train.flatten())\n",
    "plt.title('Frontera de decisión para una capa oculta de ' + str(d_1) + ' nodos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte j)\n",
    "¿Considera que el valor por defecto (1e-3) del factor de regularización es adecuado? Comente como influye este factor en la solución y cómo eligiría su valor más adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
